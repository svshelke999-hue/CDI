# Multi-Payer CDI Compliance Checker

Enhanced Multi-payer Clinical Documentation Improvement (CDI) Compliance Checker with AI-powered compliance evaluation, evidence extraction, and comprehensive caching.

## ğŸ¯ Overview

This system analyzes medical charts against multiple payer policies (Cigna, UnitedHealthcare, Anthem) to evaluate compliance and provide actionable recommendations. It includes advanced features like chart improvement, prompt caching, and multiple interface options.

## âœ¨ Key Features

- **Multi-Payer Compliance**: Simultaneous evaluation across 3 major payers
- **Flexible Data Sources**: JSON files or OpenSearch
- **Evidence Extraction**: Automatic extraction of inline citations with page/line numbers
- **Smart Caching**: Reduces API costs by 30-70% with prompt caching
- **Chart Improvement**: AI-powered medical chart enhancement
- **Multiple Interfaces**: CLI, Interactive UI, or Batch Dashboard
- **Comprehensive Output**: Detailed compliance reports with improvement recommendations

## ğŸš€ Quick Start

### 1. Install
```bash
conda env create -f environment.yml
conda activate multi-payer-cdi
```

### 2. Configure
```bash
# Set data source (choose one)
export DATA_SOURCE=json          # Simpler, no OpenSearch needed
# OR
export DATA_SOURCE=opensearch    # For large datasets

# Set AWS credentials
export AWS_REGION=us-east-1
export CLAUDE_MODEL_ID=us.anthropic.claude-3-7-sonnet-20250219-v1:0
export ENABLE_PROMPT_CACHING=true
```

### 3. Run
```bash
# Option A: CLI (automation, scripting)
python main.py chart.pdf

# Option B: Interactive UI (single file analysis)
streamlit run streamlit_app.py

# Option C: Batch Dashboard (multiple files, analytics)
streamlit run dashboard.py
```

## ğŸ—ï¸ Architecture

```
multi-payer-cdi/
â”œâ”€â”€ main.py                       # CLI entry point
â”œâ”€â”€ streamlit_app.py              # Interactive single-file UI
â”œâ”€â”€ dashboard.py                  # Batch processing dashboard
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ environment.yml               # Conda environment
â””â”€â”€ src/multi_payer_cdi/         # Core modules
    â”œâ”€â”€ core.py                   # Main orchestrator
    â”œâ”€â”€ config.py                 # Configuration
    â”œâ”€â”€ models.py                 # Data models
    â”œâ”€â”€ bedrock_client.py         # AWS Bedrock/Claude
    â”œâ”€â”€ json_loader.py            # JSON data source
    â”œâ”€â”€ opensearch_client.py      # OpenSearch integration
    â”œâ”€â”€ cache_manager.py          # Prompt caching
    â”œâ”€â”€ file_processor.py         # File handling
    â”œâ”€â”€ compliance_evaluator.py   # Compliance logic
    â””â”€â”€ chart_improver.py         # Chart improvement
```

## ğŸ“š Using the System

### Command Line Interface (CLI)

```bash
# Process single file
python main.py chart.pdf

# Process directory
python main.py /path/to/charts/

# Process multiple files
python main.py file1.pdf file2.txt file3.pdf

# Show system info
python main.py --info
```

### Streamlit UI (Single File Analysis)

```bash
streamlit run streamlit_app.py
```

**Features:**
- ğŸ“ Drag-and-drop file upload
- ğŸ“Š Real-time processing
- ğŸ¥ Multi-payer tabs (Cigna, UHC, Anthem)
- ğŸ“‹ Expandable procedure sections
- ğŸ“ˆ Interactive metrics
- ğŸ’¾ Live cache statistics
- ğŸ“¥ JSON export

### Multi-File Dashboard (Batch Processing)

```bash
streamlit run dashboard.py
```

**Features:**
- ğŸ“ Batch file upload
- ğŸ“Š Aggregate analytics
- ğŸ¥ Payer comparison
- ğŸ“ˆ Compliance trends
- ğŸ’° Cost tracking
- ğŸ“¥ CSV/JSON export

## ğŸ”§ Configuration

### Environment Variables

**AWS Settings (Required):**
```bash
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
CLAUDE_MODEL_ID=us.anthropic.claude-3-7-sonnet-20250219-v1:0
```

**Data Source Settings:**
```bash
DATA_SOURCE=json                    # "json" or "opensearch"
```

**JSON Mode Settings (if DATA_SOURCE=json):**
```bash
ANTHEM_JSON_PATH="path/to/anthem/data"
UHC_JSON_PATH="path/to/uhc/data"
CIGNA_JSON_PATH="path/to/cigna/data"
```

**OpenSearch Settings (if DATA_SOURCE=opensearch):**
```bash
OS_HOST=http://localhost:9200
OS_USER=admin
OS_PASS=admin
OS_SSL_VERIFY=false
OS_INDEX=rag-chunks
```

**Cache Settings:**
```bash
CACHE_DIR=./cache
CACHE_TTL_HOURS=24
ENABLE_CACHE=true
ENABLE_PROMPT_CACHING=true
```

## ğŸ“Š Data Sources

### JSON Mode (Recommended for Simplicity)

**Advantages:**
- âœ… No OpenSearch installation needed
- âœ… Faster startup (5-30 seconds)
- âœ… Works offline
- âœ… Easier to update (just replace files)
- âœ… Version control friendly
- âœ… Portable and self-contained

**Setup:**
```bash
export DATA_SOURCE=json
python main.py --info  # Verify paths
```

### OpenSearch Mode (Recommended for Large Datasets)

**Advantages:**
- âœ… Better for large datasets (>10K guidelines)
- âœ… Dynamic updates
- âœ… Advanced search capabilities
- âœ… Distributed architecture
- âœ… Better performance at scale

**Setup:**
```bash
# Start OpenSearch
docker run -d --name opensearch \
  -p 9200:9200 -p 9600:9600 \
  -e "discovery.type=single-node" \
  -e "DISABLE_INSTALL_DEMO_CONFIG=true" \
  -e "DISABLE_SECURITY_PLUGIN=true" \
  opensearchproject/opensearch:latest

# Configure system
export DATA_SOURCE=opensearch
export OS_HOST=http://localhost:9200
```

## ğŸ¯ Chart Improvement Feature

The system includes an AI-powered chart improvement feature that analyzes CDI recommendations and generates enhanced medical charts.

### How It Works

1. **Process Chart** â†’ Upload and analyze through CDI system
2. **Review Recommendations** â†’ See compliance gaps from all payers
3. **Generate Improvement** â†’ AI creates improved chart with guidance
4. **Review & Edit** â†’ Use 5 interactive tabs to refine
5. **Export Final Chart** â†’ Download for EHR submission

### Features

- **5 Interactive Tabs:**
  - ğŸ“„ Improved Chart - View and edit enhanced chart
  - âœï¸ Fields Needing Input - Priority-based field completion
  - ğŸ“ Improvements Made - Before/after comparisons
  - ğŸ’¡ Recommendations - Categorized action items
  - â†”ï¸ Compare Original vs Improved - Side-by-side view

- **AI Capabilities:**
  - Reorganizes information for clarity
  - Marks additions with `[ADDED: ...]` tags
  - Marks required inputs with `[NEEDS PHYSICIAN INPUT: ...]` tags
  - Provides specific guidance for missing fields
  - Never fabricates clinical data

### Cost Impact

- **Additional LLM Call:** Only 1 extra call per chart
- **Typical Cost:** $0.005 - $0.015 per improvement
- **Time Savings:** 60-90 minutes per chart (manual improvement time)

## ğŸ’° Prompt Caching

The system includes advanced prompt caching to reduce API costs by 40-50%.

### How It Works

1. **First Call:** Creates cache (25% more expensive)
2. **Subsequent Calls:** Read from cache (90% cheaper!)
3. **Cache TTL:** 5 minutes of inactivity

### Configuration

```bash
ENABLE_PROMPT_CACHING=true
CLAUDE_MODEL_ID=us.anthropic.claude-3-7-sonnet-20250219-v1:0
MIN_CACHE_TOKENS=1024
```

### Test Caching

```bash
python test_prompt_caching.py
```

Expected output:
```
âœ… PROMPT CACHING IS WORKING CORRECTLY!
   Cost Reduction: 44.2%
   Ready for production use!
```

## ğŸ” Evidence & References

The system extracts inline evidence citations from guideline data:

### Format
```
(Evidence: pg no: 2, L73)           # Page 2, Line 73
(Evidence: pg no: 10, L468-L480)    # Page 10, Lines 468-480
```

### Usage
1. **Verify Requirements:** Note reference, open payer guideline PDF, verify requirement
2. **Prepare Appeals:** Review procedure results, check PDF Evidence tab, include in appeal

## ğŸ¯ Use Cases

- **Clinical Documentation Review**: Identify documentation gaps before claim submission
- **Appeal Preparation**: Get evidence-backed support for insurance appeals
- **Quality Improvement**: Analyze patterns across multiple charts
- **Training**: Educate staff on payer-specific requirements
- **Compliance Auditing**: Ensure documentation meets payer standards

## ğŸ”§ System Requirements

- Python 3.11+
- AWS Account with Bedrock access
- 4GB+ RAM
- Optional: Docker (for OpenSearch)

## ğŸ“Š Performance

- **Processing Time**: 30-90 seconds per file (first run), 10-30 seconds (cached)
- **Cost**: $0.02-0.05 per file (varies by complexity)
- **Cache Savings**: 30-70% cost reduction on similar charts
- **Prompt Caching**: Additional 40-50% savings

## ğŸ”’ Security & Privacy

- Charts processed in memory only
- No permanent storage of PHI
- AWS Bedrock security standards
- Automatic file cleanup after processing

## ğŸ› Troubleshooting

### Common Issues

**1. "No JSON guideline files found"**
```bash
python main.py --info  # Verify paths
export ANTHEM_JSON_PATH="/new/path/to/anthem/data"
```

**2. "OpenSearch connection failed"**
```bash
curl http://localhost:9200  # Check if running
docker start opensearch     # Start if needed
```

**3. "AWS Bedrock authentication failed"**
```bash
aws sts get-caller-identity  # Verify credentials
```

**4. "Processing is slow"**
```bash
export ENABLE_CACHE=true
export ENABLE_PROMPT_CACHING=true
```

**5. "Unicode/Emoji errors in console"**
```powershell
$env:PYTHONIOENCODING="utf-8"
```

## ğŸ“ˆ Cost Management

**Expected Costs:**
- First run: $0.02-0.05 per file
- Cached runs: 30-50% reduction
- High similarity: 60-70% reduction
- With prompt caching: Additional 40-50% savings

**Budget Planning:**
```bash
# Calculate monthly costs
average_cost_per_file=$0.03
files_per_month=1000
monthly_cost=$(echo "$average_cost_per_file * $files_per_month" | bc)
echo "Estimated monthly cost: \$$monthly_cost"
```

## ğŸ§ª Testing

### Sample Charts

**1. Mixed Chart (CPT + Procedures):**
```bash
python main.py sample_chart_mixed.txt
```

**2. Procedure-Only Chart:**
```bash
python main.py sample_chart_procedure_only.txt
```

### Test Prompt Caching
```bash
python test_prompt_caching.py
```

## ğŸ“ License

See LICENSE file for details.

## ğŸ¤ Support

For detailed help and troubleshooting, refer to the system logs in the `logs/` directory.

---

**Version 1.0.0** | Built with AWS Bedrock & Claude AI | All documentation consolidated into a single comprehensive guide